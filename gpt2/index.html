<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fine-Tuning amb Google Colab</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }
        h1, h2, h3 { color: #2c3e50; }
        pre { background: #f4f4f4; padding: 10px; border-radius: 5px; overflow-x: auto; }
        code { font-family: "Courier New", monospace; color: #d63384; }
    </style>
</head>
<body>

    <h1>ğŸ“Œ Guia de Fine-Tuning amb Google Colab</h1>
    <p>En aquesta guia aprendrÃ s a utilitzar <strong>Google Colab</strong> per fer <strong>fine-tuning d'un model preentrenat</strong> com <strong>GPT-2</strong> o <strong>Mistral</strong> amb els teus propis documents (com PDFs de drogues vegetals i medicaments a base de plantes).</p>

    <h2>ğŸ”¹ Per quÃ¨ Google Colab?</h2>
    <p>Google Colab et permet utilitzar <strong>GPU gratuÃ¯tes</strong> per realitzar tasques d'IA com el <strong>fine-tuning</strong> de models sense necessitat d'un ordinador potent. Amb <strong>4 GB de RAM</strong> al teu PC local, Colab serÃ  una bona opciÃ³.</p>

    <h2>ğŸ“Œ PAS 1: Crear un Notebook de Google Colab</h2>
    <p>1. Ves a [Google Colab](https://colab.research.google.com/).</p>
    <p>2. Crea un nou notebook seleccionant <strong>"New Notebook"</strong>.</p>
    <p>3. Connecta't a un <strong>GPU</strong> fent clic a <strong>"Runtime" > "Change runtime type"</strong> i seleccionant <strong>GPU</strong> com a dispositiu de hardware.</p>

    <h2>ğŸ“Œ PAS 2: InstalÂ·lar les llibreries necessÃ ries</h2>
    En el teu notebook de Colab, primer has d'instalÂ·lar les llibreries necessÃ ries per fer <strong>fine-tuning</strong>. Afegeix el segÃ¼ent codi a la primera celÂ·la del notebook:

    <pre><code>
    !pip install transformers datasets accelerate bitsandbytes
    </code></pre>

    AixÃ² instalÂ·larÃ :
    - <strong>transformers</strong> per carregar i treballar amb models de Hugging Face.
    - <strong>datasets</strong> per carregar els teus conjunts de dades.
    - <strong>accelerate</strong> per optimitzar l'entrenament.
    - <strong>bitsandbytes</strong> per carregar models de gran mida amb menys memÃ²ria.

    <h2>ğŸ“Œ PAS 3: Preparar el conjunt de dades</h2>
    Abans de fer el fine-tuning, necessites convertir els teus PDFs a text. Per aixÃ², utilitza el segÃ¼ent codi per extreure el text dels teus PDFs i convertir-los en un conjunt de dades utilitzable.

    <pre><code>
    from pypdf import PdfReader
    import os

    carpeta_pdfs = "pdfs_ema"  # O la carpeta on tinguis els PDFs
    sortida_txt = "dataset.txt"

    with open(sortida_txt, "w", encoding="utf-8") as fitxer:
        for arxiu in os.listdir(carpeta_pdfs):
            if arxiu.endswith(".pdf"):
                lector = PdfReader(os.path.join(carpeta_pdfs, arxiu))
                for pÃ gina in lector.pages:
                    fitxer.write(pÃ gina.extract_text() + "\n")
    </code></pre>

    AixÃ² extraurÃ  tot el text dels PDFs i el guardarÃ  en el fitxer `dataset.txt`.

    <h2>ğŸ“Œ PAS 4: Cargar i preparar el model</h2>
    Carrega un model petit per fer el fine-tuning. Pots utilitzar <strong>GPT-2</strong> per comenÃ§ar:

    <pre><code>
    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
    from datasets import load_dataset

    model_id = "gpt2"  # Model petit per a fine-tuning
    dataset = load_dataset("text", data_files="dataset.txt")

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)  # Usar 8 bits per reduir la memÃ²ria
    </code></pre>

    AixÃ² carregarÃ  el model <strong>GPT-2</strong> i la tokenitzaciÃ³ de les dades.

    <h2>ğŸ“Œ PAS 5: Tokenitzar les dades</h2>
    Abans de poder entrenar el model, cal tokenitzar les dades (convertir el text a una forma comprensible per al model). Utilitza aquest codi:

    <pre><code>
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

    tokenized_datasets = dataset.map(tokenize_function, batched=True)
    </code></pre>

    AixÃ² convertirÃ  el text del teu dataset en el format adequat per al model.

    <h2>ğŸ“Œ PAS 6: Configurar l'entrenament</h2>
    Ara configura l'entrenament, especificant el nombre d'epochs, la mida del lot i altres parÃ metres:

    <pre><code>
    training_args = TrainingArguments(
        output_dir="./gpt2_finetuned",
        per_device_train_batch_size=1,  # Menor batch size per reduir la memÃ²ria
        gradient_accumulation_steps=8,  # Acumula gradients per simular un batch mÃ©s gran
        evaluation_strategy="epoch",
        save_strategy="epoch",
        num_train_epochs=3,
        learning_rate=2e-5
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"]
    )

    trainer.train()
    </code></pre>

    AixÃ² entrena el model durant <strong>3 epochs</strong> i guarda el model entrenat a la carpeta `gpt2_finetuned`.

    <h2>ğŸ“Œ PAS 7: Desar el model entrenat</h2>
    Un cop completat l'entrenament, pots desar el model i carregar-lo mÃ©s tard:

    <pre><code>
    trainer.save_model("gpt2_finetuned")
    model = AutoModelForCausalLM.from_pretrained("gpt2_finetuned")
    </code></pre>

    <h2>ğŸ“Œ PAS 8: Provar el model entrenat</h2>
    Ara pots provar el model generat per veure com respon a les teves preguntes:

    <pre><code>
    entrada = tokenizer("Quina Ã©s la planta medicinal mÃ©s usada?", return_tensors="pt")
    sortida = model.generate(**entrada)
    print(tokenizer.decode(sortida[0], skip_special_tokens=True))
    </code></pre>

    AixÃ² generarÃ  una resposta a la pregunta.

    <h2>ğŸ“Œ PAS 9: Crear un chatbot amb el model entrenat</h2>
    Utilitza el model entrenat per construir un chatbot senzill amb el codi segÃ¼ent:

    <pre><code>
    import torch

    def chatbot(input_text):
        entrada = tokenizer(input_text, return_tensors="pt")
        sortida = model.generate(**entrada)
        return tokenizer.decode(sortida[0], skip_special_tokens=True)

    while True:
        user_input = input("Preguntar sobre drogues vegetals o medicaments: ")
        if user_input.lower() == "sortir":
            break
        print("Resposta: ", chatbot(user_input))
    </code></pre>

    <h2>ğŸ“Œ ConclusiÃ³</h2>
    Amb <strong>Google Colab</strong>, pots fer <strong>fine-tuning de models d'IA</strong> de manera senzilla i amb <strong>poques despeses</strong> de recursos locals. Utilitzant models petits com <strong>GPT-2</strong> o fins i tot <strong>DistilGPT-2</strong>, pots entrenar un model per a la teva tasca (com respondre preguntes sobre drogues vegetals o medicaments). TambÃ© pots carregar models mÃ©s grans al nÃºvol per fer <strong>fine-tuning</strong> sense necessitat d'una GPU potent localment.

</body>
</html>
